{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Restart the Jupyter Kernel before running the code every single time so that the current directory will reset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from IPython.display import JSON\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.expanduser(\"../\"))\n",
    "\n",
    "from dees_package.youtube_functions import *\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_search(any_youtube, max_results: int, query: str, searchtype: str, region: str, category: int):\n",
    "    search_data = []\n",
    "    video_ids = []\n",
    "\n",
    "    next_page_token = None\n",
    "\n",
    "    while True :\n",
    "        youtube_search_request = any_youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            maxResults=min(50, max_results),  # Maximum allowed value is 50\n",
    "            q=query,\n",
    "            type=searchtype,\n",
    "            regionCode = region,\n",
    "            videoCategoryId=category,\n",
    "            order=\"viewCount\",\n",
    "            fields=\"items(id/videoId,snippet(channelId,channelTitle,description,title)),nextPageToken,pageInfo,prevPageToken,regionCode\",\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "\n",
    "        # Execute the request and get the response\n",
    "        youtube_search_response = youtube_search_request.execute()\n",
    "        print(youtube_search_response['pageInfo']['totalResults'])\n",
    "\n",
    "        # iterate through each element in the nested dictionary to get the relevant values of each video\n",
    "        for item in youtube_search_response.get('items', []):\n",
    "            video_id = item['id']['videoId']\n",
    "            title = item['snippet']['title']\n",
    "            channel_id = item['snippet']['channelId']\n",
    "            channel_title = item['snippet']['channelTitle']\n",
    "\n",
    "            # append the relevant values to the data dictionary to save as a dataframe\n",
    "            search_data.append({\n",
    "                'video_id': video_id,\n",
    "                'title': title,\n",
    "                'channel_id': channel_id,\n",
    "                'channel_title': channel_title,\n",
    "            })\n",
    "\n",
    "            video_ids.append(video_id)\n",
    "\n",
    "        # Check if there are more pages\n",
    "        next_page_token = youtube_search_response.get('nextPageToken')\n",
    "        if not next_page_token or len(video_ids) >= max_results:\n",
    "            break  # No more pages or reached the desired number of results\n",
    "\n",
    "    # Return the collected data and video IDs\n",
    "    return search_data, video_ids\n",
    "\n",
    "\n",
    "def get_stats(any_youtube, videoId:list):\n",
    "\n",
    "    # video_ids_str = ','.join(videoId) # pre-2024 code, video ID as comma separated strings; but apparently it's ok to just use a list now\n",
    "    video_data= []\n",
    "\n",
    "\n",
    "    # create the request object\n",
    "    # from the above response, we already have the channelId, channelTitle, videoID, categoryID, \n",
    "    chunk_size = 50\n",
    "    for i in range(0, len(videoId), chunk_size):\n",
    "        current_chunk = videoId[i:i+chunk_size-1]\n",
    "        video_request = any_youtube.videos().list(\n",
    "        part=\"statistics, id, topicDetails, contentDetails\",\n",
    "        id=\",\".join(current_chunk))\n",
    "\n",
    "        video_response = video_request.execute()\n",
    "\n",
    "        print(len(video_response['items']))\n",
    "        pprint(video_response['items'])\n",
    "\n",
    "    # iterate through each element in the nested dictionary to get the relevant values\n",
    "        for item in video_response['items']:\n",
    "            view_count = item['statistics']['viewCount'] if 'viewCount' in item['statistics'] else None\n",
    "            like_count = item['statistics']['likeCount'] if 'likeCount' in item['statistics'] else None \n",
    "            comment_count = item['statistics']['commentCount'] if 'commentCount' in item['statistics'] else None\n",
    "            wikipedia_category = item['topicDetails']['topicCategories'] if 'topicCategories' in item['topicDetails'] else None\n",
    "            duration = item['contentDetails']['duration']\n",
    "\n",
    "\n",
    "            # append the relevant values to the data dictionary to save as a dataframe\n",
    "            video_data.append ({\n",
    "            'video_id': item['id'],\n",
    "            'view_count': view_count,\n",
    "            'like_count': like_count,\n",
    "            'comment_count': comment_count,\n",
    "            'wikipedia_categories': wikipedia_category,\n",
    "            'duration': duration\n",
    "            })\n",
    "            print(len(video_data))\n",
    "        \n",
    "    return video_data\n",
    "\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    \n",
    "    for i in range (0,len(video_ids),1):\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_ids[i]\n",
    "            )\n",
    "            response = request.execute()\n",
    "            print(response)\n",
    "           \n",
    "            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]] \n",
    "            comments_in_video_info = {'video_id':video_ids[i], 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Could not get comments for video {i}. Error: {e}')\n",
    "        continue # Skip to the next iteration in case of an error\n",
    "        \n",
    "    return pd.DataFrame(all_comments)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file_path = \"./credentials.json\"\n",
    "\n",
    "# open the file and load the data into a variable\n",
    "with open(credentials_file_path, \"r\") as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating service object of the youtube version 3 API\n",
    "service_youtube = build('youtube', 'v3', developerKey=credentials['youtube_api_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of music videos, carried out using .search().list() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_search_data, video_id = youtube_search(service_youtube, 2000, \"official music video\", \"video\", \"US\", 10)\n",
    "\n",
    "with open('./data/yt_search_data.json', 'w') as json_file:\n",
    "    json.dump(youtube_search_data, json_file, indent=4)\n",
    "\n",
    "yt_search_df = pd.DataFrame(youtube_search_data)\n",
    "yt_search_df.to_csv('./data/search.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = get_stats(service_youtube, video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting statistics on each video, using video IDs from previous function as an input, carried out using .videos().list() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_stats = get_stats(service_youtube, video_id)  # there is a limit on the number of video ids, can only run 50 at a time. Solution: create different lists with 50 IDs each.\n",
    "video_stats_df = pd.DataFrame(video_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the mv stats and search dataframes\n",
    "merged_df = pd.merge(yt_search_df, video_stats_df, left_on='video_id', right_on='video_id')\n",
    "merged_df.to_json('./data/merged.json')\n",
    "\n",
    "merged_df.to_csv('./data/merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting comments, carried out using .commentThreads().list() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = get_comments_in_videos(service_youtube, video_id) # note that comments are disabled for some videos\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final merge of dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_youtube_df = pd.merge(merged_df, comments_df, left_on='video_id', right_on='video_id', sort = False)\n",
    "final_youtube_df.to_csv('./data/final_youtube.csv')\n",
    "final_youtube_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
