{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Set-up:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the additional packages to be installed on the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install scikit-learn\n",
    "pip install rpy2\n",
    "pip install spotipy\n",
    "pip install nltk\n",
    "pip install wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages\n",
    "\n",
    "*⚠️ Note: Do not run this more than once. Restart the kernel before running this code chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from plotnine import *\n",
    "import plotnine as p9\n",
    "import re\n",
    "from scrapy import Selector\n",
    "import requests as requests\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "import spotipy\n",
    "import base64\n",
    "from requests import post\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from datetime import datetime\n",
    "from sklearn import *\n",
    "from base64 import *\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.expanduser(\"../\"))                 # change directory to main project directory\n",
    "\n",
    "from dees_package.spotify_functions import *  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we are in the correct current working directory\n",
    "\n",
    "*⚠️ Note: We should be in the main project directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open JSON file containing credentials\n",
    "\n",
    "*⚠️ Note: Our credentials should be stored in a file titled `credentials.json` and stored in the root of the project folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file_path = './credentials.json'\n",
    "\n",
    "with open(credentials_file_path, 'r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Dataframe from merged YouTube Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **For Ruikai:**\n",
    "So all the raw and (almost fully) cleaned data that we get from YouTube + Genius is in this csv file called raw_compiled_data.csv\n",
    "So i edited a lil bit of ur code to take out some of the cleaning that was like already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df = pd.read_csv('../data/raw_compiled_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under 'wikipedia_categories', there are separate links for different potential genres\n",
    "\n",
    "We have noticed that:\n",
    "* Each link is separated by a comma ','\n",
    "* Every song has at least one category – 'music'\n",
    "* Some songs are in multiple categories, majority of them only has only one, some has two, songs with two and more categories are extremely rare\n",
    "\n",
    "Therefore, we can count the number of commas to determine the number of categories, with the function as such:\n",
    "\n",
    "# **IM SO SORRY RUIKAI BUT UR GONNA NEED TO REDO THIS ONE**\n",
    "The wikipedia categories are all in one string so this doesnt run lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_number(x):\n",
    "    string = str(x)\n",
    "    return string.count(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge['category_number'] = new_merge['wikipedia_categories'].apply(lambda x: get_category_number(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge2 = new_merge.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_merge = new_merge.head(200)\n",
    "big_merge['lyrics'] = big_merge.apply(lambda row: scrape_lyrics(my_session, row['Genius_URL']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge2['lyrics'] = new_merge2.apply(lambda row: scrape_lyrics(my_session, row['Genius_URL']), axis=1)\n",
    "\n",
    "new_merge2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Analyse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a new dataframe with the necessary headers only, removing 'None' values or duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge3 = new_merge2.dropna()\n",
    "\n",
    "df = new_merge3[['Artist', 'Song', 'like_count', 'view_count', 'comment_count', 'lyrics', 'category_number']].dropna().drop_duplicates(subset = ['Song'])\n",
    "\n",
    "df = df[df['lyrics'] != '']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported package to analyse sentiments\n",
    "\n",
    "We create function and apply it to dataframe|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(lyric):\n",
    "    scores = sid.polarity_scores(lyric)\n",
    "    list = [scores['neg'], scores['neu'], scores['pos'], scores['compound']]\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_positive'] = df['lyrics'].apply(lambda x: get_sentiment_score(x)[2])\n",
    "df['sentiment_neutral'] = df['lyrics'].apply(lambda x: get_sentiment_score(x)[1])\n",
    "df['sentiment_negative'] = df['lyrics'].apply(lambda x: get_sentiment_score(x)[0])\n",
    "df['sentiment_compound'] = df['lyrics'].apply(lambda x: get_sentiment_score(x)[3])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define lexical richness as the proportion of unique words to total words used, a quantitative way to analyse the richness of vocabulary used in a song. Using function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_richness(lyric):\n",
    "    total_words = len(lyric.split())\n",
    "    unique_words = len(set(lyric.split()))\n",
    "    lexical_richness = unique_words/total_words*100\n",
    "    return round(lexical_richness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lexical_richness'] = df['lyrics'].apply(lambda x: get_lexical_richness(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find song length as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['song_length'] = df['lyrics'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_compound_absolute'] = df['sentiment_compound'].abs()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Spotify API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we integrate spotify API as well to find even more categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = credentials['client_id']\n",
    "client_secret = credentials['client_secret']\n",
    "\n",
    "client_creds = f\"{client_id}:{client_secret}\"\n",
    "base64_client_creds = b64encode(client_creds.encode()).decode()\n",
    "\n",
    "auth_url = 'https://accounts.spotify.com/api/token'\n",
    "headers = {\n",
    "    'Authorization': f'Basic {base64_client_creds}'\n",
    "}\n",
    "payload = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "response = requests.post(auth_url, headers=headers, data=payload)\n",
    "\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 'spotipy' package and the search() function, we are able to get data in the json file such as release date, a popularity score, whether the song is explicit, and the number of markets that the song is in during its initial release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating these into our existing dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_date'] = df['Song'].apply(lambda x: get_release_date(x, client_id, client_secret))\n",
    "df['popularity'] = df['Song'].apply(lambda x: get_popularity(x, client_id, client_secret))\n",
    "df['explicitness'] = df['Song'].apply(lambda x: get_explicitness(x, client_id, client_secret))\n",
    "df['markets'] = df['Song'].apply(lambda x: get_market_number(x, client_id, client_secret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to convert our date to datetime format for ease of plotting later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(x):\n",
    "    try:\n",
    "        pd.to_datetime(x)\n",
    "        return pd.to_datetime(x)\n",
    "    except:\n",
    "        None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_date'] = df['release_date'].apply(lambda x: convert_date(x)).dropna()\n",
    "# df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the number of markets of song release, we found some interesting facts:\n",
    "\n",
    "For an initial release of song, it is in either:\n",
    "* all 184 markets in the world\n",
    "* slightly less than 184 markets (a sign that there are some censorship in some countries, a hint that the song may be culturally inappropriate/politically sensitive)\n",
    "* or very little markets (<50) (a sign that the song is deliberately only released in some markets, targeting niche categories)\n",
    "\n",
    "Hence justifying the below function, categorising them into high, medium, or low level of outreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_availability_category(x):\n",
    "    number = int(x)\n",
    "    if number == 184:\n",
    "        return 'High'\n",
    "    elif 50 < number < 184:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['markets'] = df['markets'].apply(lambda x: market_availability_category(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, for song categories:\n",
    "\n",
    "We initially attempted to obtain song genres via YouTube, Genius or Spotify. However, we faced significant difficulties due to the fact that:\n",
    "* The data is not explicitly available – these platforms offer limited sources of data to public due to privacy reasons\n",
    "* It is very difficult to get the genre via the API itself\n",
    "\n",
    "Therefore, we enlisted Wikipedia, an open source, to find out on the song genre/category. However, due to the limited amount of categorisations there are on Wikipedia, we focus on the number of categories, i.e. number of wikipedia pages they occur instead.\n",
    "* Most songs do not belong to any specific category on Wikipedia, they are being categorised as \"music\".\n",
    "* For most of the other songs, they belong to two Wikipedia categories, \"music\" and something else, such as \"electro\"\n",
    "* The rest of the songs are extreme minorities which belongs to three or more Wikipedia categories\n",
    "\n",
    "Hence justifying our rationale to have broad categories. Songs that are not relevant enough to have more than one genre are categorised as \"Low\" in terms of category popularity; two as \"Medium\", three or more as \"High\". The function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_popularity(x):\n",
    "    number = int(x)\n",
    "    if number == 1:\n",
    "        return 'Low'\n",
    "    elif number == 2:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_number'] = df['category_number'].apply(lambda x: category_popularity(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(df.iloc[0,5])\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df[['like_count','view_count','comment_count', 'sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'sentiment_compound_absolute', 'lexical_richness', 'song_length', 'popularity']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df2 = corr_df. \\\n",
    "        melt(ignore_index=False) \\\n",
    "        .reset_index()\n",
    "\n",
    "corr_df2['rounded_value'] = corr_df2['value'].apply(lambda x: np.round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = p9.ggplot(\n",
    "        mapping = p9.aes('index', 'variable', fill = 'value'),\n",
    "        data = corr_df2\n",
    "    ) + \\\n",
    "        p9.geom_tile() + \\\n",
    "        p9.geom_label(\n",
    "            p9.aes(label = 'rounded_value'),\n",
    "            fill = 'white',\n",
    "            size = 8\n",
    "        ) + \\\n",
    "        p9.scale_fill_distiller() + \\\n",
    "        p9.theme_minimal() + \\\n",
    "        p9.labs(\n",
    "            title = 'Correlation Matrix',\n",
    "            x = '',\n",
    "            y = ''\n",
    "        ) + \\\n",
    "        p9.theme(\n",
    "            axis_text_x = element_text(angle = 90)\n",
    "        )\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = p9.ggplot(\n",
    "    mapping = p9.aes(x = 'sentiment_compound'),\n",
    "    data = df\n",
    ") + \\\n",
    "geom_histogram(binwidth=0.05)\n",
    "\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = (\n",
    "    ggplot(df) +\n",
    "    aes(x = 'explicitness', y = 'popularity') +\n",
    "    geom_boxplot()\n",
    ")\n",
    "\n",
    "boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = (\n",
    "    ggplot(df) +\n",
    "    aes(x = 'release_date', y = 'song_length', colour = 'explicitness') +\n",
    "    geom_point(alpha = 0.5) +\n",
    "    geom_smooth(method = \"lm\") +\n",
    "    scale_x_datetime(\n",
    "        limits=(datetime(2000, 1, 1), datetime(2024, 1, 1)),\n",
    "    )\n",
    ")\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = (\n",
    "    ggplot(df) +\n",
    "    aes(x = 'popularity', y = 'song_length') +\n",
    "    geom_bin2d() +\n",
    "    theme_classic()\n",
    ")\n",
    "\n",
    "contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"../data/json_for_plot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = (\n",
    "    ggplot(df) +\n",
    "    aes(x = 'lexical_richness', y = 'sentiment_compound', z = 'popularity') +\n",
    "    geom_contour_filled(aes(fill = 'level') +\n",
    "    geom_contour(colour = 'black'))\n",
    ")\n",
    "\n",
    "contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = (\n",
    "    ggplot(df) +\n",
    "    aes(x = 'popularity', colour = 'category_number', fill = 'category_number') +\n",
    "    geom_density(alpha = 0.2)\n",
    ")\n",
    "\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages(\"IRkernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages(\"IRkernel\")\n",
    "plot = (\n",
    "    ggplot(df, aes(x='lexical_richness', y='sentiment_compound', z='popularity')) +\n",
    "    geom_contour_filled(aes(fill='..level..')) +\n",
    "    geom_contour(color='black') +\n",
    "    scale_fill_cmap(name='viridis')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
